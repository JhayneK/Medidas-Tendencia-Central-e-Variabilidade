# -*- coding: utf-8 -*-
"""Medidas-Tendencia-Central-e-Variabilidade.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O2MwBZjvWrv7nRUdHxD1GDuSgYMKE-JD
"""

#01) Exemplo 1 da confeitaria
!pip -q install pandas numpy matplotlib xlsxwriter openpyxl

# -*- coding: utf-8 -*-
"""
Análise operacional de confeitaria:
- Lê dados de um XLSX/CSV (ou simula, se o arquivo não existir)
- Calcula média/mediana do tempo de entrega
- Monitora desvio-padrão (previsibilidade)
- Compara lojas por CV (%)
- Gera gráficos e exporta um Excel com resumos
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# ========== CONFIG ==========
# Caminhos de entrada (troque conforme necessário)
CANDIDATES = [
    "confeitaria.xlsx",
    "confeitaria.csv",
    "operacoes.xlsx",
    "operacoes.csv",
]

# Caminho de saída do Excel (troque se quiser salvar em outro lugar)
OUT_XLSX = "confeitaria_relatorio.xlsx"

# Meta de estabilidade (ex.: reduzir desvio-padrão para <= 8 min)
TARGET_STD = 8.0  # minutos

# ========== FUNÇÕES ==========
def _try_first_available(paths):
    """Retorna o primeiro caminho existente em 'paths' ou None se nenhum existir."""
    for p in paths:
        if os.path.exists(p):
            return p
    return None

def _read_df(path: str) -> pd.DataFrame:
    """Lê XLSX/CSV em DataFrame."""
    if path.endswith(".xlsx"):
        return pd.read_excel(path)
    return pd.read_csv(path)

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Colunas em minúsculas/sem espaços extras."""
    return df.rename(columns={c: str(c).strip().lower() for c in df.columns})

# ========== CARGA DE DADOS ==========
path = _try_first_available(CANDIDATES)

if path:
    raw = _read_df(path)
else:
    # Simulação: 30 dias, 2 lojas (A/B), 40 entregas por dia/loja
    np.random.seed(7)
    days = pd.date_range(datetime.today().date() - timedelta(days=29), periods=30, freq="D")
    recs = []
    for d in days:
        # Loja A: média ~34 min; temperatura ~8.5°C
        times_a = np.clip(np.random.normal(34, 6, 40), 8, None)
        temps_a = np.random.normal(8.5, 1.2, 40)
        # Loja B: média ~36 min; mais instável
        times_b = np.clip(np.random.normal(36, 9, 40), 8, None)
        temps_b = np.random.normal(9.0, 1.6, 40)
        for t, temp in zip(times_a, temps_a):
            recs.append({"data": d, "loja": "A", "tempo_min": float(t), "temperatura_c": float(temp)})
        for t, temp in zip(times_b, temps_b):
            recs.append({"data": d, "loja": "B", "tempo_min": float(t), "temperatura_c": float(temp)})
    raw = pd.DataFrame.from_records(recs)

df = _normalize_columns(raw).copy()

# Mapeamento flexível de colunas (aceita variações comuns)
data_col = next((c for c in df.columns if c in ["data", "dia", "day", "date"]), None)
tempo_col = next((c for c in df.columns if c in ["tempo_min", "tempo", "min", "tempo_entrega", "tempo_entrega_min"]), None)
temp_col  = next((c for c in df.columns if c in ["temperatura_c", "temp_c", "temperatura", "temp"]), None)
loja_col  = next((c for c in df.columns if c in ["loja", "filial", "store"]), None)

# Defaults e conversões
if data_col is None:
    data_col = "data"
    df[data_col] = pd.to_datetime("today").date()
else:
    df[data_col] = pd.to_datetime(df[data_col], errors="coerce").dt.date

if tempo_col is None:
    tempo_col = "tempo_min"
    # tenta usar a 1ª coluna numérica como fallback
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    if num_cols:
        df[tempo_col] = pd.to_numeric(df[num_cols[0]], errors="coerce")
    else:
        raise ValueError("Não foi possível identificar coluna de tempo. Informe 'tempo_min' ou equivalente.")
else:
    df[tempo_col] = pd.to_numeric(df[tempo_col], errors="coerce")

if temp_col is None:
    temp_col = "temperatura_c"
    df[temp_col] = np.nan
else:
    df[temp_col] = pd.to_numeric(df[temp_col], errors="coerce")

if loja_col is None:
    loja_col = "loja"
    df[loja_col] = "A"

# Limpa linhas inválidas
df = df.dropna(subset=[data_col, tempo_col]).copy()

# ========== RESUMOS ==========
# 1) Overal diário: média/mediana/std (ddof=1) e CV%
daily = df.groupby(data_col)[tempo_col].agg(
    mean="mean",
    median="median",
    std=lambda s: s.std(ddof=1),
    count="count"
)
daily["cv_percent"] = (daily["std"] / daily["mean"]) * 100

# 2) Resumo geral (promessa por mediana/média arredondadas para múltiplos de 5)
promessa_mediana = int(5 * round(np.median(df[tempo_col]) / 5.0))
promessa_media   = int(5 * round(np.mean(df[tempo_col]) / 5.0))
resumo_operacao = pd.DataFrame([{
    "periodo": f"{df[data_col].min()} a {df[data_col].max()}",
    "media_tempo_min": round(df[tempo_col].mean(), 2),
    "mediana_tempo_min": round(df[tempo_col].median(), 2),
    "desvio_padrao_tempo_min": round(df[tempo_col].std(ddof=1), 2),
    "cv_percent_global": round((df[tempo_col].std(ddof=1) / df[tempo_col].mean()) * 100, 2),
    "promessa_por_mediana_min": promessa_mediana,
    "promessa_por_media_min": promessa_media
}])

# 3) Por loja: média, mediana, std, CV% + temperatura média/std (se houver)
by_store = df.groupby(loja_col).agg(
    count=(tempo_col, "count"),
    tempo_medio=(tempo_col, "mean"),
    tempo_mediana=(tempo_col, "median"),
    tempo_std=(tempo_col, lambda s: s.std(ddof=1)),
    temp_media=(temp_col, "mean"),
    temp_std=(temp_col, lambda s: s.std(ddof=1)),
)
by_store["cv_percent"] = (by_store["tempo_std"] / by_store["tempo_medio"]) * 100
by_store = by_store.round(2).sort_values("cv_percent")

# 4) Metas: baseline do std diário, % de dias já dentro da meta
baseline_std = float(daily["std"].mean()) if not daily.empty else float("nan")
pct_dias_ok = float((daily["std"] <= TARGET_STD).mean() * 100) if not daily.empty else float("nan")
metas = pd.DataFrame([{
    "desvio_padrao_baseline_min": round(baseline_std, 2),
    "meta_desvio_padrao_min": TARGET_STD,
    "dias_em_que_std<=meta_%": round(pct_dias_ok, 1)
}])

# ========== GRÁFICOS ==========
# 1) CV diário (todas as lojas)
plt.figure()
plt.plot(daily.index, daily["cv_percent"])
plt.title("CV diário (%) do tempo de entrega (todas as lojas)")
plt.xlabel("Dia")
plt.ylabel("CV (%)")
plt.xticks(rotation=45)
plt.tight_layout()
cv_png = "confeitaria_cv_diario.png"
plt.savefig(cv_png, dpi=160)
plt.close()

# 2) Boxplot por loja (distribuição do tempo)
plt.figure()
ordem_lojas = by_store.index.tolist()
data_by_store = [df.loc[df[loja_col] == lo, tempo_col].dropna().values for lo in ordem_lojas]
plt.boxplot(data_by_store, labels=ordem_lojas, showfliers=False)
plt.title("Distribuição do tempo de entrega por loja")
plt.xlabel("Loja")
plt.ylabel("Tempo (min)")
plt.tight_layout()
box_png = "confeitaria_box_por_loja.png"
plt.savefig(box_png, dpi=160)
plt.close()

# ========== EXCEL ==========
with pd.ExcelWriter(OUT_XLSX, engine="xlsxwriter") as writer:
    df.to_excel(writer, index=False, sheet_name="Dados_Brutos")
    resumo_operacao.to_excel(writer, index=False, sheet_name="Resumo_Operacao")
    daily.round(2).reset_index().to_excel(writer, index=False, sheet_name="Resumo_Diario")
    by_store.reset_index().to_excel(writer, index=False, sheet_name="Resumo_Lojas")
    metas.to_excel(writer, index=False, sheet_name="Metas")

    # Inserir imagens (se existirem)
    ws1 = writer.sheets["Resumo_Diario"]
    if os.path.exists(cv_png):
        ws1.insert_image("H2", cv_png)

    ws2 = writer.sheets["Resumo_Lojas"]
    if os.path.exists(box_png):
        ws2.insert_image("H2", box_png)

print("\n=== RELATÓRIO GERADO ===")
print(f"Arquivo Excel: {OUT_XLSX}")
print(f"Gráfico CV diário: {cv_png}")
print(f"Boxplot por loja: {box_png}")

# Dicas de leitura:
# - Use a MEDIANA para prometer prazo típico (menos sensível a picos).
# - Monitore o DESVIO-PADRÃO e o CV%: se subirem, a operação está menos previsível.
# - Compare lojas pelo CV%: menor CV = mais consistência proporcional.
# - Defina metas: ex. reduzir std de X -> {TARGET_STD} min e acompanhe % de dias dentro da meta.

uploaded = files.upload()  # selecione um ou mais arquivos; eles irão para /content
print("Arquivos recebidos:", list(uploaded.keys()))

# =========================
# 1) EXERCÍCIO (dados/Ex.)
# =========================
ex_df = _try_load(EXERCICIO_FILE)

if ex_df is not None:
    # tenta identificar coluna relevante
    col_candidates = [c for c in ex_df.columns if str(c).lower() in ["tempo_min", "valor", "tempo", "dados"]]
    if not col_candidates:
        numeric_cols = ex_df.select_dtypes(include=["number"]).columns.tolist()
        col_candidates = numeric_cols[:1]
    if not col_candidates:
        # fallback para exemplo
        exercise_data = [10, 12, 12, 14, 16, 18, 18, 20]
        ex_df = pd.DataFrame({"tempo_min": exercise_data})
        ex_col = "tempo_min"
    else:
        ex_col = col_candidates[0]
else:
    # exemplo padrão
    exercise_data = [10, 12, 12, 14, 16, 18, 18, 20]
    ex_df = pd.DataFrame({"tempo_min": exercise_data})
    ex_col = "tempo_min"

ex_summary = summarize(ex_df[ex_col])
display(ex_summary.round(2))

# histograma
fig1 = plt.figure()
plt.hist(ex_df[ex_col].dropna(), bins="auto")
plt.title("Exercício: Distribuição (histograma)")
plt.xlabel(ex_col)
plt.ylabel("Frequência")
png_hist_ex = "/content/exercicio_hist.png"
save_plot_png(fig1, png_hist_ex)

# =============================
# 2) OPERAÇÕES (dados/Simulação)
# =============================
ops_raw = _try_load(OPERACOES_FILE)

if ops_raw is None:
    # Simulação de 30 dias * 50 entregas/dia (3 regimes de variabilidade)
    np.random.seed(42)
    days = pd.date_range(datetime.today().date() - timedelta(days=29), periods=30, freq="D")

    def simulate_day(mean, std, n=50):
        x = np.random.normal(mean, std, n)
        x = np.clip(x, 5, None)
        return x

    recs = []
    for i, day in enumerate(days, start=1):
        if i <= 10:
            data = simulate_day(35, 5, 50)
        elif i <= 20:
            data = simulate_day(36, 8, 50)
        else:
            data = simulate_day(38, 12, 50)
        for v in data:
            recs.append({"day": day, "tempo_entrega_min": float(v)})
    ops_df = pd.DataFrame.from_records(recs)
else:
    cols_lower = {c: str(c).lower() for c in ops_raw.columns}
    ops_raw.rename(columns=cols_lower, inplace=True)

    day_col = next((c for c in ops_raw.columns if "day" in c or "data" in c), None)
    tempo_col = next((c for c in ops_raw.columns if "tempo_entrega_min" in c or "tempo" in c or "min" in c), None)
    if day_col is None or tempo_col is None:
        day_col = ops_raw.columns[0]
        tempo_col = ops_raw.columns[1] if len(ops_raw.columns) > 1 else ops_raw.columns[0]

    ops_df = ops_raw[[day_col, tempo_col]].copy()
    ops_df[day_col] = pd.to_datetime(ops_df[day_col], errors="coerce").dt.date
    ops_df.rename(columns={day_col: "day", tempo_col: "tempo_entrega_min"}, inplace=True)
    ops_df["tempo_entrega_min"] = pd.to_numeric(ops_df["tempo_entrega_min"], errors="coerce")
    ops_df.dropna(subset=["day", "tempo_entrega_min"], inplace=True)

daily = ops_df.groupby("day")["tempo_entrega_min"].agg(
    mean="mean",
    median="median",
    std=lambda s: s.std(ddof=1),
    count="count"
)
daily["cv_percent"] = (daily["std"] / daily["mean"]) * 100
alerts = daily[daily["cv_percent"] > ALERT_THRESHOLD].copy()

display(daily.round(2))

# gráfico CV diário
fig2 = plt.figure()
plt.plot(daily.index, daily["cv_percent"])
plt.axhline(ALERT_THRESHOLD)
plt.title(f"Operações: CV diário (%) do tempo de entrega (limiar {ALERT_THRESHOLD}%)")
plt.xlabel("Dia")
plt.ylabel("CV (%)")
plt.xticks(rotation=45)
png_cv_ops = "/content/operacoes_cv_diario.png"
save_plot_png(fig2, png_cv_ops)

# =======================
# 3) DEV (latência A x B)
# =======================
lat_raw = _try_load(LATENCIA_FILE)

if lat_raw is None:
    # Simula duas versões (A/B) com 10k requisições
    n_req = 10000
    lat_a = np.random.lognormal(mean=np.log(180), sigma=0.4, size=n_req)
    lat_b = np.random.lognormal(mean=np.log(175), sigma=0.3, size=n_req)
    lat_df = pd.DataFrame({
        "versao": ["A"] * n_req + ["B"] * n_req,
        "lat_ms": np.concatenate([lat_a, lat_b])
    })
else:
    cols_lower = {c: str(c).lower() for c in lat_raw.columns}
    lat_raw.rename(columns=cols_lower, inplace=True)
    vcol = next((c for c in lat_raw.columns if "versao" in c or "version" in c), None)
    lcol = next((c for c in lat_raw.columns if "lat_ms" in c or "latencia" in c or "latency" in c), None)
    if vcol is None or lcol is None:
        vcol = lat_raw.columns[0]
        lcol = lat_raw.columns[1] if len(lat_raw.columns) > 1 else lat_raw.columns[0]
    lat_df = lat_raw[[vcol, lcol]].copy()
    lat_df.rename(columns={vcol: "versao", lcol: "lat_ms"}, inplace=True)
    lat_df["lat_ms"] = pd.to_numeric(lat_df["lat_ms"], errors="coerce")
    lat_df.dropna(subset=["versao", "lat_ms"], inplace=True)
    lat_df["versao"] = lat_df["versao"].astype(str).str.upper().str.strip()

def latency_summary(df: pd.DataFrame, label: str) -> dict:
    s = pd.to_numeric(df["lat_ms"], errors="coerce").dropna()
    return {
        "versao": label,
        "count": int(s.count()),
        "mean_ms": s.mean(),
        "median_ms": s.median(),
        "p95_ms": s.quantile(0.95),
        "std_ms": s.std(ddof=1),
        "cv_percent": (s.std(ddof=1) / s.mean()) * 100 if s.mean() != 0 else np.nan,
    }

lat_summary = pd.DataFrame([
    latency_summary(lat_df[lat_df["versao"]=="A"], "A"),
    latency_summary(lat_df[lat_df["versao"]=="B"], "B"),
]).round(2)
display(lat_summary)

# histograma (escala log)
lat_a_vals = lat_df[lat_df["versao"]=="A"]["lat_ms"].values
lat_b_vals = lat_df[lat_df["versao"]=="B"]["lat_ms"].values

fig3 = plt.figure()
plt.hist(lat_a_vals, bins=60, alpha=0.5, label="A")
plt.hist(lat_b_vals, bins=60, alpha=0.5, label="B")
plt.xscale("log")
plt.title("Distribuição de latência (ms) - A vs. B")
plt.xlabel("Latência (ms, escala log)")
plt.ylabel("Frequência")
plt.legend()
png_lat_hist = "/content/latencia_hist.png"
save_plot_png(fig3, png_lat_hist)

# =========================
# 4) Exporta para o Excel
# =========================
with pd.ExcelWriter(OUT_XLSX, engine="xlsxwriter") as writer:
    # Exercício
    ex_df.to_excel(writer, index=False, sheet_name="Exercicio_Dados")
    ex_summary.to_excel(writer, index=False, sheet_name="Exercicio_Resumo")
    ws = writer.sheets["Exercicio_Resumo"]
    if os.path.exists(png_hist_ex):
        ws.insert_image("H2", png_hist_ex)

    # Operações
    ops_df.to_excel(writer, index=False, sheet_name="Operacoes_Dados")
    daily.round(2).reset_index().to_excel(writer, index=False, sheet_name="Operacoes_Resumo")
    ws2 = writer.sheets["Operacoes_Resumo"]
    if os.path.exists(png_cv_ops):
        ws2.insert_image("H2", png_cv_ops)

    # Latência
    lat_df.to_excel(writer, index=False, sheet_name="Latencia_Dados")
    lat_summary.to_excel(writer, index=False, sheet_name="Latencia_Resumo")
    ws3 = writer.sheets["Latencia_Resumo"]
    if os.path.exists(png_lat_hist):
        ws3.insert_image("H2", png_lat_hist)

print("Arquivo Excel gerado em:", OUT_XLSX)

# =========================
# 5) Conclusões automáticas
# =========================
conclusoes = []

# Exercício
try:
    ex_cv = float(ex_summary["cv_percent"].iloc[0])
    if ex_cv < 15:
        ex_msg = "Baixa dispersão: processo consistente."
    elif ex_cv < 30:
        ex_msg = "Dispersão moderada: variação relevante, porém controlável."
    else:
        ex_msg = "Alta dispersão: processo instável; investigar causas."
    conclusoes.append(f"[Exercício] CV={ex_cv:.1f}% → {ex_msg}")
except Exception as e:
    conclusoes.append(f"[Exercício] Não foi possível calcular conclusões ({e}).")

# Operações
ops_cv_mean = float(daily["cv_percent"].mean())
ops_alert_days = alerts.shape[0]
conclusoes.append(
    f"[Operações] CV médio diário={ops_cv_mean:.1f}% com {ops_alert_days} dia(s) acima do limiar de {ALERT_THRESHOLD}%."
)

# Dev
try:
    cv_a = float(lat_summary.loc[lat_summary["versao"] == "A", "cv_percent"].values[0])
    cv_b = float(lat_summary.loc[lat_summary["versao"] == "B", "cv_percent"].values[0])
    p95_a = float(lat_summary.loc[lat_summary["versao"] == "A", "p95_ms"].values[0])
    p95_b = float(lat_summary.loc[lat_summary["versao"] == "B", "p95_ms"].values[0])
    conclusoes.append(
        f"[Dev] CV(A)={cv_a:.1f}% → CV(B)={cv_b:.1f}%. P95(A)={p95_a:.0f}ms → P95(B)={p95_b:.0f}ms. "
        f"{'Melhorou' if (cv_b < cv_a and p95_b <= p95_a) else 'Piorou/Estável'} em previsibilidade."
    )
except Exception as e:
    conclusoes.append(f"[Dev] Não foi possível comparar versões ({e}).")

print("\n".join(conclusoes))

# 02) Exemplo 2 das idades
# Exercício: medidas de tendência central e variabilidade
import numpy as np
import pandas as pd
from collections import Counter

idades = np.array([25, 28, 30, 32, 35, 40], dtype=float)

# Média
media = idades.mean()

# Mediana
mediana = np.median(idades)

# Moda (tratando o caso "sem moda")
contagem = Counter(idades.tolist())
max_freq = max(contagem.values())
if max_freq == 1:
    moda_desc = "não há moda"
else:
    moda_vals = sorted([int(k) for k, v in contagem.items() if v == max_freq])
    moda_desc = f"moda(s): {moda_vals}"

# Amplitude
amplitude = int(idades.max() - idades.min())

# Resultado final
resultado = pd.DataFrame([{
    "dados": [25, 28, 30, 32, 35, 40],
    "média": round(media, 2),     # 31.67
    "mediana": round(mediana, 2), # 31.00
    "moda": moda_desc,            # "não há moda"
    "amplitude": amplitude        # 15
}])

resultado